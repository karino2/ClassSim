{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "basemodel_layer_num = 311 #corresponding to len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH=\"trained_model\"\n",
    "%mkdir -p $BASE_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modelutils import ModelCompiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler = ModelCompiler(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = \"bay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"{}/model_{}\".format(BASE_MODEL_PATH, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 12s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = compiler.generate_compiled_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.processor import create_generators\n",
    "\n",
    "TRAIN_DATAGEN, VALID_DATAGEN = create_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelutils' is different extension.\n",
    "\n",
    "def dir2filedict(basedir):\n",
    "    res = {}\n",
    "    for f in glob.iglob(\"{}/**/*\".format(basedir), recursive=True):\n",
    "        cat = os.path.basename(os.path.dirname(f))\n",
    "        res.setdefault(cat, []).append(f)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdict = dir2filedict(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "catecories = sorted(fdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bay',\n",
       " 'beach',\n",
       " 'birds',\n",
       " 'boeing',\n",
       " 'buildings',\n",
       " 'city',\n",
       " 'clouds',\n",
       " 'data',\n",
       " 'f-16',\n",
       " 'face',\n",
       " 'helicopter',\n",
       " 'mountain',\n",
       " 'ocean',\n",
       " 'ships',\n",
       " 'sky',\n",
       " 'sunrise',\n",
       " 'sunset']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catecories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VALID_RATIO=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files(input_paths, ratio= TRAIN_VALID_RATIO):\n",
    "    paths = sorted(input_paths)\n",
    "    random.shuffle(paths)\n",
    "    sep = int(len(paths)*ratio)\n",
    "    return paths[0:sep], paths[sep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fdict(fdict):\n",
    "    trdict = {}\n",
    "    valdict = {}\n",
    "    cats = sorted(fdict.keys())\n",
    "    for cat in cats:\n",
    "        tr, val = split_files(fdict[cat])\n",
    "        trdict[cat] = tr\n",
    "        valdict[cat] = val\n",
    "    return trdict, valdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "trdict, valdict = split_fdict(fdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from one_vs_all.py for development purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.one_vs_all import OneVsAllFilesIterator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import json\n",
    "\n",
    "#Image resize size\n",
    "SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "FilesPair = collections.namedtuple(\"FilesPair\", [\"trues\", \"falses\"])\n",
    "TrValFiles = collections.namedtuple('TrValFiles', ['trainings', 'valids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import Iterator\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVsAllFilesIterator(Iterator):\n",
    "    def __init__(self, true_files, false_files, image_data_generator, target_size=(256, 256), batch_size=32, shuffle=True, seed=None):\n",
    "\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "\n",
    "        # assume channel last.\n",
    "        self.data_format = 'channels_last'\n",
    "        assert backend.image_data_format() == self.data_format\n",
    "        self.image_shape = self.target_size + (3,)\n",
    "\n",
    "        self.filenames = []\n",
    "        self.filenames.extend(true_files)\n",
    "        self.classes = np.ones(len(true_files))\n",
    "        self.filenames.extend(false_files)\n",
    "        self.classes = np.append(self.classes, np.zeros(len(false_files)))\n",
    "\n",
    "        self.n = len(self.filenames)\n",
    "\n",
    "        super(OneVsAllFilesIterator, self).__init__(self.n, batch_size, shuffle, seed)\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=backend.floatx())\n",
    "        for i, j in enumerate(index_array):\n",
    "            fname = self.filenames[j]\n",
    "            img = load_img(fname,\n",
    "                           grayscale=False,\n",
    "                           target_size=self.target_size)\n",
    "            x = img_to_array(img, data_format=self.data_format)\n",
    "            x = self.image_data_generator.random_transform(x)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "        batch_y = self.classes[index_array].astype(backend.floatx())\n",
    "        return batch_x, batch_y\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVsAllModelTrainer:\n",
    "    def __init__(self, train_datagen, valid_datagen):\n",
    "        self.train_datagen = train_datagen\n",
    "        self.valid_datagen = valid_datagen\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    def set_savepath(self, model_save_path):\n",
    "        self.model_save_path= model_save_path\n",
    "        self.file_path = self.model_save_path + \"-{epoch:02d}-{val_acc:.3f}.h5\"\n",
    "        self.checkpoint = ModelCheckpoint(\n",
    "            self.file_path\n",
    "            , monitor='val_acc'\n",
    "            , verbose=1\n",
    "            , save_best_only=False\n",
    "            , mode='max'\n",
    "        )\n",
    "        self.callbacks_list = [self.checkpoint]\n",
    "    def set_dataset(self, trvals):\n",
    "        self.trvals = trvals\n",
    "    def set_dataset_files(self, true_trainings, false_trainings, true_valids, false_valids):\n",
    "        trs = FilesPair(trues=true_trainings, falses=false_trainings)\n",
    "        vals = FilesPair(trues=true_valids, falses = false_valids)\n",
    "        trval = TrValFiles(trs, vals)\n",
    "        self.set_dataset(trval)\n",
    "    def validation_generator(self, batch_size, target_size):\n",
    "\n",
    "        # false_sampled = random.sample(false_valids, len(true_valids))\n",
    "        # temporary use whole false validation data.\n",
    "        vals = self.trvals.valids\n",
    "        false_sampled = vals.falses\n",
    "        return OneVsAllFilesIterator(vals.trues, false_sampled, self.valid_datagen, target_size=target_size, batch_size=batch_size)\n",
    "    def save_result(self, history):\n",
    "        # use epoch 99 as special (last saved model).\n",
    "        self.model.save_weights(\"{0}-99-{1:.3f}.h5\".format(self.model_save_path, history.history['val_acc'][-1]))\n",
    "    def list_checkpoints_except_best(self):\n",
    "        pat = \"{}-*.h5\".format(self.model_save_path)\n",
    "        paths = list(glob.iglob(pat))\n",
    "        best = choose_best_val_acc_path(paths)\n",
    "        return [path for path in paths if path != best]\n",
    "    def remove_checkpoint(self):\n",
    "        list(map(os.remove, self.list_checkpoints_except_best()))\n",
    "    def train_model(self, eachepochs=5, batch_size=16, target_size=(SIZE, SIZE)):\n",
    "        with open(\"{0}.json\".format(self.model_save_path), 'w') as f:\n",
    "            json.dump(json.loads(self.model.to_json()), f) # model.to_json() is a STRING of json\n",
    "\n",
    "        trs = self.trvals.trainings\n",
    "\n",
    "        validgen = self.validation_generator(batch_size, target_size)\n",
    "        traingen = OneVsAllFilesIterator(trs.trues, random.sample(trs.falses, len(trs.trues)),  self.train_datagen, target_size=target_size, batch_size= batch_size)\n",
    "\n",
    "        history = self.model.fit_generator(\n",
    "            generator=traingen\n",
    "            #, steps_per_epoch= 100\n",
    "            , steps_per_epoch= traingen.n/batch_size\n",
    "            , epochs=eachepochs\n",
    "            , verbose=1\n",
    "            , validation_data=validgen\n",
    "            , validation_steps=validgen.n/batch_size\n",
    "            # , validation_steps=10\n",
    "            , callbacks=self.callbacks_list\n",
    "        )\n",
    "\n",
    "        self.save_result(history)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.category import split_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = OneVsAllModelTrainer(TRAIN_DATAGEN, VALID_DATAGEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_model(model)\n",
    "trainer.set_savepath(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train, false_train = split_files(cat, trdict)\n",
    "true_valid, false_valid = split_files(cat, valdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_dataset_files(true_train, false_train, true_valid, false_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12/98 [==>...........................] - ETA: 48:25 - loss: 0.7004 - acc: 0.5885"
     ]
    }
   ],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
