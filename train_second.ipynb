{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train second set of one vs. rest (OVR) classifiers.\n",
    "\n",
    "We train another set of classifiers that are used for classifications.  \n",
    "These classifiers are trained using similar images for each target class; similarities between classes are computed in *classifier_similarity.ipynb*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH=\"trained_model\"\n",
    "%mkdir -p $BASE_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modelutils import ModelCompiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler = ModelCompiler(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.processor import create_generators\n",
    "\n",
    "TRAIN_DATAGEN, VALID_DATAGEN = create_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modelutils import dir2filedict, split_fdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load category and file path information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdict = dir2filedict(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = sorted(fdict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data int {train, validation, test} datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdict, testdict = split_fdict(fdict, test_size=0.2, random_state = 123)\n",
    "trdict, valdict = split_fdict(trdict, test_size=0.2, random_state = 456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdict['clouds'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is expected outputs.   \n",
    "The output may be different if you create image urls yourself or exlude some files for GMM, but all the outputs in {*train.ipynb*, *classifier_similarity.ipynb*, *train_multiclass_classifier.ipynb*, *train_second.ipynb*} must be the same. \n",
    "\n",
    "['data/clouds/0678.jpeg',  \n",
    " 'data/clouds/0701.jpeg',  \n",
    " 'data/clouds/0431.jpeg',  \n",
    " 'data/clouds/0033.jpeg',  \n",
    " 'data/clouds/0290.jpeg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train second level classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class for training second level classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.one_vs_all import OneVsAllModelTrainer\n",
    "from models.modelutils import split_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = OneVsAllModelTrainer(TRAIN_DATAGEN, VALID_DATAGEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.one_vs_all import FilesPair, TrValFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLevelClassifierTrainer:\n",
    "    def __init__(self, base_model_name, basedir, trainer, compiler):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.basedir = basedir\n",
    "\n",
    "        self.compiler = compiler\n",
    "        self.trainer = trainer\n",
    "        \n",
    "    def setup_filedict(self, train_files_dict, valid_files_dict):\n",
    "        self.train_files_dict = train_files_dict\n",
    "        self.valid_files_dict = valid_files_dict\n",
    "        self.valid_files_dict_org = self.valid_files_dict\n",
    "        \n",
    "    def _model_path(self, target_key):\n",
    "        return os.path.join(self.basedir, \"{}_{}\".format(self.base_model_name, target_key))\n",
    "    \n",
    "    def _split_by_set(self, target_key, false_keyset, files_dict):\n",
    "        trues = files_dict[target_key]\n",
    "        falses = [path for key in false_keyset for path in files_dict[key]]\n",
    "        return FilesPair(trues, falses)\n",
    "    \n",
    "    def _split_files(self, targetkey, files_dict):\n",
    "        return FilesPair(*split_files(targetkey, files_dict))\n",
    "    \n",
    "    def train_second_level(self, target_key, highcat_keyset, eachepochs=10, retrainings=1, removecheckpoint=True):\n",
    "        self.trainer.retrainings = retrainings\n",
    "        falseset = highcat_keyset - set(target_key)\n",
    "        trs = self._split_by_set(target_key, falseset, self.train_files_dict)\n",
    "        vals = self._split_by_set(target_key, falseset, self.valid_files_dict)\n",
    "        trvals = TrValFiles(trs, vals)\n",
    "        self._train_one_core(\"sec_\"+target_key, trvals, eachepochs, removecheckpoint)\n",
    "        \n",
    "    def _train_one_setup(self, model_key, trvals):\n",
    "        model_save_path = self._model_path(model_key)\n",
    "\n",
    "        model = self.compiler.generate_compiled_model(model_save_path)\n",
    "        self.trainer.set_model(model)\n",
    "        self.trainer.set_savepath(model_save_path)\n",
    "        self.trainer.set_dataset(trvals)\n",
    "\n",
    "    def _train_one_core(self, model_key, trvals, eachepochs, removecheckpoint):\n",
    "        self._train_one_setup(model_key, trvals)\n",
    "\n",
    "        self.trainer.train_model(eachepochs=eachepochs)\n",
    "        if removecheckpoint:\n",
    "            self.trainer.remove_checkpoint()\n",
    "\n",
    "    def remove_checkpoint(self, model_key):\n",
    "        # utility method for cleaup interrupted case\n",
    "        self.trainer.set_savepath(self._model_path(model_key))\n",
    "        self.trainer.remove_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_trainer = SecondLevelClassifierTrainer(\"model\", BASE_MODEL_PATH, trainer, compiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_trainer.setup_filedict(trdict, valdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load $ClassSim$ results to gather similar classes for each target class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classsim = pd.read_pickle(\"results/valid_sim_df.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seconds(keys, eachepochs=5):\n",
    "    for targetkey in keys:\n",
    "        similarkeyset = set(classsim[targetkey][classsim[targetkey] >= SIM_THRESHOLD].index)\n",
    "        try:\n",
    "            if len(similarkeyset) == 1:\n",
    "                print(\"no similar category. only first classifier is enough. skip second training.\")\n",
    "            else:\n",
    "                sec_trainer.train_second_level(targetkey, similarkeyset, eachepochs=eachepochs)\n",
    "        except ValueError as e:\n",
    "            print(\"ValueError, skip {0}: {1}\".format(targetkey, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seconds(categories[0:], eachepochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
