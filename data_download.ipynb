{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from urls\n",
    "\n",
    "Get data from urls obtained in *datascraping.ipynb*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cencepts from paper\n",
    "\n",
    "Concepts from Table 1 in [Measuring Semantic Similarity between Concepts in Visual Domain](http://ieeexplore.ieee.org/document/4665152/).  \n",
    "Concepts are corresponding to categories in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [\n",
    "    \"bay\",\n",
    "    \"beach\",\n",
    "    \"birds\",\n",
    "    \"boeing\",\n",
    "    \"buildings\",\n",
    "    \"city\",\n",
    "    \"clouds\",\n",
    "    \"face\",\n",
    "    \"f-16\",\n",
    "    \"helicopter\",\n",
    "    \"mountain\",\n",
    "    \"sky\",\n",
    "    \"ships\",\n",
    "    \"sunset\",\n",
    "    \"sunrise\",\n",
    "    \"ocean\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images using obtained list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define several functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_urls(cat):\n",
    "    with open(\"urls/{}.txt\".format(cat), encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.rsplit('\\n')[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted = {\"jpeg\", \"png\", \"gif\"}\n",
    "\n",
    "def get_one(dirname, urlidx, urls):\n",
    "    url = urls[urlidx]\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        ext = response.info().get_content_subtype()\n",
    "        if not ext in accepted:\n",
    "            return False\n",
    "    urllib.request.urlretrieve(url, \"{}/{:04d}.{}\".format(dirname, urlidx, ext))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_many_no_error_handle(urls, dirname):\n",
    "    with futures.ThreadPoolExecutor(MAX_WORKERS) as executor:\n",
    "        res = executor.map(lambda idx: get_one(dirname, idx, urls), range(len(urls)))\n",
    "    _ = len(list(res))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from  http.client import RemoteDisconnected\n",
    "from http.client import HTTPException\n",
    "import ssl\n",
    "\n",
    "def download_many(urls, dirname):\n",
    "    to_do_map = {}\n",
    "    fails = []\n",
    "    with futures.ThreadPoolExecutor(MAX_WORKERS) as executor:\n",
    "        for i in range(len(urls)):\n",
    "            future = executor.submit(get_one,\n",
    "                            dirname, i, urls)\n",
    "            to_do_map[future] = i\n",
    "        done_iter = futures.as_completed(to_do_map)\n",
    "        \n",
    "        done_iter = tqdm.tqdm(done_iter, total=len(urls))\n",
    "        \n",
    "        notify_err = lambda msg: None\n",
    "        \n",
    "        \n",
    "        for future in done_iter:\n",
    "            idx = to_do_map[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if not res:\n",
    "                    notify_err(\"Unknown mime type: {}\".format(urls[idx]))\n",
    "                    fails.append(urls[idx])\n",
    "            # make exception handling separately for debug purpose (now we can merge, but not yet)\n",
    "            except (urllib.error.HTTPError, RemoteDisconnected, ssl.CertificateError or OSError, UnicodeEncodeError, urllib.error.URLError):\n",
    "                notify_err(\"urllib retrieve rrorr: {}\".format(urls[idx]))\n",
    "                fails.append(urls[idx])\n",
    "            except HTTPException:\n",
    "                notify_err(\"HTTPException rrorr: {}\".format(urls[idx]))\n",
    "                fails.append(urls[idx])\n",
    "                \n",
    "    return fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_one_category(cat):\n",
    "    urls = read_urls(cat)\n",
    "    dirname = \"data/{}\".format(cat)\n",
    "    \n",
    "    !mkdir -p $dirname\n",
    "    fail_urls = download_many(urls, dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute donwload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(map(download_one_category, concepts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup corrupted images\n",
    "\n",
    "Remove files that cannot be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "BROKEN = 'corrupted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_broken(f):\n",
    "    moveddir = os.path.dirname(os.path.join(BROKEN, f))\n",
    "    os.path.isdir(moveddir) or os.makedirs(moveddir)\n",
    "    shutil.copy(f, moveddir)\n",
    "    os.remove(f)\n",
    "    print(\"{} is broken, move it.\".format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"data\"\n",
    "TARGET_SIZE=(256, 256)\n",
    "\n",
    "os.makedirs(BROKEN, exist_ok=True)\n",
    "\n",
    "for f in glob.iglob(\"{}/**/*.*\".format(TARGET), recursive=True):\n",
    "    try:\n",
    "        _ = load_img(f, grayscale=False,\n",
    "                           target_size=TARGET_SIZE)\n",
    "    except:\n",
    "        move_to_broken(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
